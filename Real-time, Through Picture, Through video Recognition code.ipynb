{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31420e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from mtcnn import MTCNN\n",
    "from tkinter import Tk, Button, Label, filedialog, messagebox, Canvas, StringVar, OptionMenu  # Add StringVar and OptionMenu from tkinter\n",
    "from PIL import Image, ImageTk  # Import Image and ImageTk from PIL\n",
    "from plyer import notification\n",
    "import os\n",
    "import threading\n",
    "import winsound\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Load the trained emotion detection model for file recognition\n",
    "emotion_model = load_model('C:/Users/amc/emotion_model3.h5')\n",
    "\n",
    "# Map emotion labels to human-readable names\n",
    "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "\n",
    "# Initialize Tkinter\n",
    "root = Tk()\n",
    "root.title(\"Emotion Detection\")\n",
    "\n",
    "# Global variables\n",
    "recognition_active = False\n",
    "notification_flag = True\n",
    "last_notification_time = 0\n",
    "cooldown_duration = 5  # Cooldown duration for notifications (in seconds)\n",
    "\n",
    "# Beep sound configuration\n",
    "beep_frequency = 2500  # Frequency of the beep sound in Hz\n",
    "beep_duration = 100  # Duration of the beep sound in milliseconds\n",
    "\n",
    "# Initialize the SSD model for face detection\n",
    "ssd_net = cv2.dnn.readNetFromCaffe('deploy.prototxt', 'res10_300x300_ssd_iter_140000_fp16.caffemodel')\n",
    "\n",
    "# Thresholds for different recognition modes\n",
    "threshold_file_recognition = 0.2\n",
    "threshold_realtime_recognition = 0.3\n",
    "threshold_video_recognition = 0.3\n",
    "\n",
    "# Add background image\n",
    "background_image = Image.open('C:/Users/amc/BG.jpg')  # Change 'background_image.jpg' to your image file\n",
    "background_photo = ImageTk.PhotoImage(background_image)\n",
    "background_label = Label(root, image=background_photo)\n",
    "background_label.place(relwidth=1, relheight=1)  # Place the background label to cover the entire window\n",
    "\n",
    "# Load and resize the banner image\n",
    "banner_image = Image.open('C:/Users/amc/B3.jpg')  # Change 'path_to_banner_image.jpg' to your actual image path\n",
    "banner_image = banner_image.resize((600, 100))  # Adjust the size as needed\n",
    "\n",
    "# Convert the banner image to a PhotoImage object\n",
    "banner_photo = ImageTk.PhotoImage(banner_image)\n",
    "\n",
    "# Create a Label widget to display the banner image\n",
    "banner_label = Label(root, image=banner_photo)\n",
    "banner_label.pack()\n",
    "\n",
    "\n",
    "# Function to recognize emotion from an image using MTCNN for file recognition\n",
    "def recognize_emotion_file(img, file_path):\n",
    "    detector = MTCNN()\n",
    "    faces = detector.detect_faces(img)\n",
    "\n",
    "    # Check if faces are detected\n",
    "    if len(faces) == 0:\n",
    "        show_notification(\"No Faces Detected\", \"No faces were found in the selected image.\")\n",
    "        return\n",
    "\n",
    "    for result in faces:\n",
    "        x, y, w, h = result['box']\n",
    "        x, y = max(0, x), max(0, y)\n",
    "        face_roi = img[y:y + h, x:x + w]\n",
    "        gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "        resized_face = cv2.resize(gray_face, (48, 48))\n",
    "        normalized_face = resized_face.astype('float32') / 255.0\n",
    "        normalized_face = np.expand_dims(normalized_face, axis=-1)\n",
    "        emotion_probabilities = emotion_model.predict(np.expand_dims(normalized_face, axis=0))[0]\n",
    "        predicted_emotion_index = np.argmax(emotion_probabilities)\n",
    "        confidence = np.max(emotion_probabilities)\n",
    "\n",
    "        # Check if confidence exceeds threshold\n",
    "        if confidence >= threshold_file_recognition:\n",
    "            predicted_emotion = emotion_labels[predicted_emotion_index]\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            label = f\"{predicted_emotion.capitalize()} ({confidence:.2f})\"\n",
    "            cv2.putText(img, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Check for desired emotion\n",
    "            if predicted_emotion == \"Angry\" and notification_flag and is_notification_cooldown_passed():\n",
    "                show_notification(\"Angry Emotion Detected\", f\"Person is {predicted_emotion}!\")\n",
    "                play_beep_sound()\n",
    "                set_notification_cooldown()\n",
    "\n",
    "    # Display the annotated image with the option to save\n",
    "    display_image_with_save_option(img, file_path)\n",
    "\n",
    "\n",
    "# Function to display the annotated image with the option to save\n",
    "def display_image_with_save_option(img, file_path):\n",
    "    cv2.imshow('Emotion Detection from Image', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Ask the user if they want to save the result\n",
    "    result = messagebox.askyesno(\"Save Result\", \"Do you want to save the annotated image?\")\n",
    "    if result:\n",
    "        # Save the annotated image with detected emotions\n",
    "        save_path = save_results_folder(file_path, \"Picture results\")\n",
    "        cv2.imwrite(save_path, img)\n",
    "        show_notification(\"Result Saved\", f\"Annotated image saved as {save_path}\")\n",
    "\n",
    "\n",
    "# Function to perform emotion recognition from a selected file\n",
    "def recognize_from_file():\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Image File\", filetypes=[(\"Image files\", \".jpg;.png\")])\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is not None:\n",
    "            recognize_emotion_file(image, file_path)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", \"Failed to open the selected file.\")\n",
    "    else:\n",
    "        messagebox.showinfo(\"Info\", \"No file selected.\")\n",
    "\n",
    "\n",
    "# Function to save results folder\n",
    "def save_results_folder(file_path, folder_name):\n",
    "    # Desktop path\n",
    "    desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "    # Picture results folder path\n",
    "    results_folder_path = os.path.join(desktop_path, folder_name)\n",
    "\n",
    "    # Create the folder if it doesn't exist\n",
    "    if not os.path.exists(results_folder_path):\n",
    "        os.makedirs(results_folder_path)\n",
    "\n",
    "    # Generate timestamp for the file name\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Save the annotated image with detected emotions\n",
    "    save_path = os.path.join(results_folder_path, f\"{timestamp}_annotated_{os.path.basename(file_path)}\")\n",
    "    return save_path\n",
    "\n",
    "\n",
    "# Function to validate file type\n",
    "def validate_file_type(file_path, allowed_extensions):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    return file_extension.lower() in allowed_extensions\n",
    "\n",
    "\n",
    "# Function to start real-time recognition\n",
    "def start_realtime_recognition():\n",
    "    global recognition_active\n",
    "    recognition_active = True\n",
    "    threading.Thread(target=update_frame_realtime).start()\n",
    "\n",
    "\n",
    "# Function to update the frame for real-time recognition using SSD\n",
    "def update_frame_realtime():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    frame_number = 0  # Counter to keep track of frames\n",
    "\n",
    "    # Create directory for real-time results on desktop\n",
    "    desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "    realtime_results_path = os.path.join(desktop_path, 'real-time_results')\n",
    "    if not os.path.exists(realtime_results_path):\n",
    "        os.makedirs(realtime_results_path)\n",
    "\n",
    "    while recognition_active:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        ssd_net.setInput(blob)\n",
    "        detections = ssd_net.forward()\n",
    "\n",
    "        detected_faces = []\n",
    "\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > threshold_realtime_recognition:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(int)\n",
    "                face_roi = frame[startY:endY, startX:endX]\n",
    "\n",
    "                # Convert face ROI to grayscale\n",
    "                gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize face ROI to match model input size\n",
    "                resized_face = cv2.resize(gray_face, (48, 48))\n",
    "\n",
    "                # Normalize the resized face ROI\n",
    "                normalized_face = resized_face.astype('float32') / 255.0\n",
    "\n",
    "                # Expand dimensions to match model input shape\n",
    "                normalized_face = np.expand_dims(normalized_face, axis=-1)\n",
    "                normalized_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "                detected_faces.append((normalized_face, (startX, startY, endX, endY)))\n",
    "\n",
    "        if detected_faces:\n",
    "            emotion_predictions = emotion_model.predict(np.vstack([face[0] for face in detected_faces]))\n",
    "            for i, (normalized_face, (startX, startY, endX, endY)) in enumerate(detected_faces):\n",
    "                predicted_emotion_index = np.argmax(emotion_predictions[i])\n",
    "                confidence = emotion_predictions[i][predicted_emotion_index]\n",
    "                if confidence >= threshold_realtime_recognition:\n",
    "                    predicted_emotion = emotion_labels[predicted_emotion_index]\n",
    "                    label = f\"{predicted_emotion.capitalize()} ({confidence:.2f})\"\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                    # Check for desired emotion\n",
    "                    selected_emotion_value = selected_emotion.get()\n",
    "                    if predicted_emotion == selected_emotion_value:\n",
    "                        # Save frame where desired emotion is detected\n",
    "                        frame_path = os.path.join(realtime_results_path, f\"frame_{frame_number}.jpg\")\n",
    "                        cv2.imwrite(frame_path, frame)\n",
    "                        frame_number += 1\n",
    "\n",
    "                        if notification_flag and is_notification_cooldown_passed():\n",
    "                            show_notification(f\"{selected_emotion_value} Emotion Detected\", f\"Person is {selected_emotion_value}!\")\n",
    "                            play_beep_sound()\n",
    "                            set_notification_cooldown()\n",
    "\n",
    "        cv2.imshow('Real-time Emotion Detection', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Function to start video recognition\n",
    "def start_video_recognition():\n",
    "    global recognition_active\n",
    "    recognition_active = True\n",
    "    threading.Thread(target=recognize_from_video).start()\n",
    "\n",
    "\n",
    "# Function to recognize from video using SSD\n",
    "def recognize_from_video():\n",
    "    file_path = filedialog.askopenfilename(title=\"Select Video File\", filetypes=[(\"Video files\", \".mp4;.avi\")])\n",
    "    if not file_path:\n",
    "        return\n",
    "\n",
    "    cap = cv2.VideoCapture(file_path)\n",
    "    save_path = os.path.join(os.path.expanduser('~'), 'Desktop', 'Video_Results')\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    frame_number = 0\n",
    "\n",
    "    while recognition_active:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        h, w = frame.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "        ssd_net.setInput(blob)\n",
    "        detections = ssd_net.forward()\n",
    "\n",
    "        for i in range(detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > threshold_video_recognition:\n",
    "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                (startX, startY, endX, endY) = box.astype(int)\n",
    "                face_roi = frame[startY:endY, startX:endX]\n",
    "\n",
    "                # Convert face ROI to grayscale\n",
    "                gray_face = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "                # Resize face ROI to match model input size\n",
    "                resized_face = cv2.resize(gray_face, (48, 48))\n",
    "\n",
    "                # Normalize the resized face ROI\n",
    "                normalized_face = resized_face.astype('float32') / 255.0\n",
    "\n",
    "                # Expand dimensions to match model input shape\n",
    "                normalized_face = np.expand_dims(normalized_face, axis=-1)\n",
    "                normalized_face = np.expand_dims(normalized_face, axis=0)\n",
    "\n",
    "                emotion_probabilities = emotion_model.predict(normalized_face)[0]\n",
    "                predicted_emotion_index = np.argmax(emotion_probabilities)\n",
    "                confidence = np.max(emotion_probabilities)\n",
    "\n",
    "                # Check if confidence exceeds threshold\n",
    "                if confidence >= threshold_video_recognition:\n",
    "                    predicted_emotion = emotion_labels[predicted_emotion_index]\n",
    "                    label = f\"{predicted_emotion.capitalize()} ({confidence:.2f})\"\n",
    "                    cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, label, (startX, startY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                    # Check for desired emotion\n",
    "                    selected_emotion_value = selected_emotion.get()\n",
    "                    if predicted_emotion == selected_emotion_value:\n",
    "                        # Save annotated frame\n",
    "                        annotated_frame_path = os.path.join(save_path, f\"frame_{frame_number}.jpg\")\n",
    "                        cv2.imwrite(annotated_frame_path, frame)\n",
    "                        frame_number += 1\n",
    "\n",
    "                        if notification_flag and is_notification_cooldown_passed():\n",
    "                            show_notification(f\"{selected_emotion_value} Emotion Detected\", f\"Person is {selected_emotion_value}!\")\n",
    "                            play_beep_sound()\n",
    "                            set_notification_cooldown()\n",
    "\n",
    "        cv2.imshow('Emotion Detection from Video', frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    show_notification(\"Video Recognition Completed\", f\"Annotated frames saved in {save_path}\")\n",
    "\n",
    "\n",
    "# Function to detect faces using Haar cascades with OpenCV\n",
    "def detect_faces_opencv(gray):\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    return faces\n",
    "\n",
    "\n",
    "# Function to stop recognition\n",
    "def stop_recognition():\n",
    "    global recognition_active\n",
    "    recognition_active = False\n",
    "    show_notification(\"Recognition Stopped\", \"Recognition has been stopped.\")\n",
    "\n",
    "\n",
    "# Function to show a notification\n",
    "def show_notification(title, message):\n",
    "    notification_title = title\n",
    "    notification_message = message\n",
    "    notification.notify(title=notification_title, message=notification_message, app_name=\"Emotion Detection\")\n",
    "\n",
    "\n",
    "# Function to play a beep sound\n",
    "def play_beep_sound():\n",
    "    winsound.Beep(beep_frequency, beep_duration)\n",
    "\n",
    "\n",
    "# Function to set cooldown for notifications\n",
    "def set_notification_cooldown():\n",
    "    global last_notification_time\n",
    "    last_notification_time = time.time()\n",
    "\n",
    "\n",
    "# Function to check if notification cooldown has passed\n",
    "def is_notification_cooldown_passed():\n",
    "    global last_notification_time\n",
    "    current_time = time.time()\n",
    "    return current_time - last_notification_time >= cooldown_duration\n",
    "\n",
    "\n",
    "# Function to close the Tkinter window\n",
    "def close_window():\n",
    "    root.destroy()\n",
    "\n",
    "\n",
    "# Styling the interface\n",
    "root.configure(bg='#263238')\n",
    "root.geometry(\"800x800\")\n",
    "\n",
    "\n",
    "# Function to style buttons with curved edges\n",
    "def style_button(button):\n",
    "    button.config(relief='raised', bd=3, font=('Helvetica', 12, 'bold'), width=20)\n",
    "\n",
    "\n",
    "# Buttons for file recognition, real-time recognition, video recognition, stop recognition, and close window\n",
    "file_button = Button(root, text=\"Recognize from File\", command=recognize_from_file, bg='#4CAF50', fg='white')\n",
    "style_button(file_button)\n",
    "file_button.pack(pady=10)\n",
    "\n",
    "size_message = Label(root, text=\"File size should be less than 1MB\", bg='#F5F5F5', font=('Helvetica', 10))\n",
    "size_message.pack()\n",
    "\n",
    "realtime_button = Button(root, text=\"Real-time Recognition\", command=start_realtime_recognition, bg='#2196F3', fg='white')\n",
    "style_button(realtime_button)\n",
    "realtime_button.pack(pady=10)\n",
    "\n",
    "video_button = Button(root, text=\"Recognize from Video\", command=start_video_recognition, bg='#FF9800', fg='white')\n",
    "style_button(video_button)\n",
    "video_button.pack(pady=10)\n",
    "\n",
    "stop_button = Button(root, text=\"Stop Recognition\", command=stop_recognition, bg='#F44336', fg='white')\n",
    "style_button(stop_button)\n",
    "stop_button.pack(pady=10)\n",
    "\n",
    "close_button = Button(root, text=\"Close\", command=close_window, bg='#607D8B', fg='white')\n",
    "style_button(close_button)\n",
    "close_button.pack(pady=10)\n",
    "\n",
    "# Drop-down menu for selecting desired emotion\n",
    "selected_emotion = StringVar(root)\n",
    "selected_emotion.set(\"Angry\")  # Set default emotion to Angry\n",
    "emotion_label = Label(root, text=\"Select Desired Emotion:\", bg='#607D8B', fg='white', font=('Helvetica', 12))\n",
    "emotion_label.pack(pady=20, padx=50)\n",
    "emotions = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "emotion_menu = OptionMenu(root, selected_emotion, *emotions)\n",
    "emotion_menu.config(bg='#F44336', font=('Helvetica', 12), width=20)\n",
    "emotion_menu.pack(pady=20, padx=50)\n",
    "\n",
    "# Start the Tkinter main loop\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
